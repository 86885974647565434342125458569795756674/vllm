docker run --ipc=host --network=host --privileged -itd --name cyy-l --gpus device=all -v /home/cyy/lora:/lora nvcr.io/nvidia/pytorch:23.09-py3

docker exec --privileged -it cyy-l /bin/bash 

[安装 — vLLM](https://docs.vllm.ai/en/latest/getting_started/installation.html)

# vllm

所有请求全部加入self.waiting

step()

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/vllm/executor/gpu_executor.py(109)execute_model()

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/vllm/worker/worker.py(187)execute_model()

self.cache_swap(blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/vllm/worker/model_runner.py(636)execute_model()

self.prepare_input_tensors(seq_group_metadata_list)

self._prepare_prompt(seq_group_metadata_list)

use_cuda_graph=False

model_executable = self.model

hidden_states = model_executable(**execute_model_kwargs)

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/vllm/model_executor/models/opt.py(294)forward()

```
XFormersMetadata(slot_mapping=tensor([98816, 98817, 98818, 98819, 98820, 98821, 98800, 98801, 98802, 98803,
        98804, 98805, 98806, 98807, 98784, 98785, 98786, 98787, 98788, 98789,
        98768, 98769, 98770, 98771, 98772, 98773], device='cuda:0'), context_lens=tensor([0, 0, 0, 0], device='cuda:0', dtype=torch.int32), max_context_len=None, block_tables=tensor([], device='cuda:0', size=(4, 0), dtype=torch.int32), kv_cache_dtype='auto', is_prompt=True, prompt_lens=[6, 8, 6, 6], prompt_lens_tensor=tensor([6, 8, 6, 6], device='cuda:0'), num_prompt_tokens=26, num_generation_tokens=0, max_subquery_len=8, max_prompt_len=8, subquery_start_loc=tensor([ 0,  6, 14, 20, 26], device='cuda:0', dtype=torch.int32), seq_start_loc=tensor([ 0,  6, 14, 20, 26], device='cuda:0', dtype=torch.int32), use_cuda_graph=False)
```

OPTAttention:

hidden_states.shape

torch.Size([26, 2048])

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/vllm/attention/layer.py(38)forward()

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/vllm/attention/backends/xformers.py(174)forward()

PagedAttention.write_to_paged_cache(key, value, key_cache,

output = self._run_memory_efficient_xformers_forward(
                    query, key, value, attn_metadata)

attn_bias = BlockDiagonalCausalMask.from_seqlens(
                    attn_metadata.prompt_lens)

out = xops.memory_efficient_attention_forward(

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/xformers/ops/fmha/\__init__.py(231)memory_efficient_attention_forward()

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/xformers/ops/fmha/flash.py(438)apply()

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/xformers/ops/fmha/flash.py(78)_flash_fwd()

out.shape

torch.Size([26, 32, 64])

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/vllm/worker/model_runner.py:656

第一次跳过

input_tokens.shape

torch.Size([4])

is_prompt=False, use_cuda_graph=True

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/vllm/worker/model_runner.py(946)forward()

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/cuda/graphs.py(90)replay()

vLLM uses cuda graph only for decoding requests.

enforce_eager=True

```
XFormersMetadata(slot_mapping=tensor([98822, 98808, 98790, 98774], device='cuda:0'), context_lens=tensor([7, 9, 7, 7], device='cuda:0', dtype=torch.int32), max_context_len=9, block_tables=tensor([[6176],
        [6175],
        [6174],
        [6173]], device='cuda:0', dtype=torch.int32), kv_cache_dtype='auto', is_prompt=False, prompt_lens=None, prompt_lens_tensor=None, num_prompt_tokens=0, num_generation_tokens=4, max_subquery_len=None, max_prompt_len=None, subquery_start_loc=None, seq_start_loc=None, use_cuda_graph=False)
```

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/vllm/attention/backends/xformers.py(277)forward()

output = PagedAttention.forward_decode(

use_v1 = (max_context_len <= 8192
                  and (max_num_partitions == 1 or num_seqs * num_heads > 512))

```
from vllm import LLM, SamplingParams

# Sample prompts.
prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]
# Create a sampling params object.
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

# Create an LLM.
llm = LLM(model="facebook/opt-1.3b", enforce_eager=True)
# Generate texts from the prompts. The output is a list of RequestOutput objects
# that contain the prompt, generated text, and other information.
outputs = llm.generate(prompts, sampling_params, use_tqdm=False)
# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

应该拆分generate

# time

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/vllm/entrypoints/llm.py

my_add_request

my_run_prompt

my_run_decode

/root/anaconda3/envs/myenv/lib/python3.9/site-packages/vllm/engine/llm_engine.py

my_step

```
def _schedule(self) -> SchedulerOutputs:
	if not self.swapped:
		prompt
```

